{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks https://goo.gl/L0lDDp\n",
    "#### Computational Exercise - Bioelectronics and Biosensors FS 2016\n",
    "\n",
    "As seen in the lecture, convolutional neural networks are a powerful tool to, amongst other things, classify images. The most famous dataset for this purpose is the MNIST dataset, which we are going to work with. There are many different libraries out there to train neural networks, each with it's own advantages and disadvantages. A very powerful library (used by Facebook, etc.), yet simple to understand is Torch. Torch is a very powerful machine learning framework based on the scripting language Lua. If you are not familiar with Lua, don’t worry, it is very straightforward to learn. After solving this exercise you will have a basic understanding of convolutional neural networks and their applications to vision problems. You will also learn a lot about Torch. You can actually use this knowledge later for real world machine learning problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Part 0: Requirements/OS__\n",
    "\n",
    "\n",
    "If you are not using an OS with unix under the hood (i.e. Windows), do the following: Download and install Cygwin, click on curl under \"Net\" category in Cygwin package manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Part 1: Setup__\n",
    "\n",
    "First things first. You should start setting up torch and get it to run by following instructions here: http://torch.ch/docs/getting-started.html#_\n",
    "\n",
    "If you don’t want to mess up your current environment, please use a virtual environment, as described here:\n",
    "http://docs.python-guide.org/en/latest/dev/virtualenvs/\n",
    "\n",
    "The next thing we need for easy scripting and visualisation is itorch, please set it up, following this link:\n",
    "https://github.com/facebook/iTorch\n",
    "\n",
    "Well done! Now you are ready to do some fancy machine learning with torch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Part 2: Getting started__\n",
    "\n",
    "Now, to get familiar with Lua and torch, you should follow this tutorial:\n",
    "https://github.com/soumith/cvpr2015/blob/master/Deep%20Learning%20with%20Torch.ipynb\n",
    "\n",
    "Well done! You are now very close to tackling the problem.\n",
    "\n",
    "Please keep this link in mind: https://github.com/torch/torch7/wiki/Cheatsheet\n",
    "This is the whole Reference for the torch framework. For many of the following tasks you will find many useful functions in there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Part 3: MNIST__ \n",
    "\n",
    "As mentioned earlier, we want now to start on the (former) goldstandard of image classification, the MNIST dataset. As used in the torch tutorial, we will start off by implementing the LeNet 5 network. It was the first breakthrough, of using convolutional neural networks for image classification. If you are interested, have a look at the paper, which will explain many details(including the architecture) in depth: http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We start by loading the mnist dataset in 32x32 geometry. As mentioned in the tutorial, you should normalize your data by it having a mean of 0 and a standard deviation of 1. This is already done for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require 'torch'\n",
    "require 'paths'\n",
    "\n",
    "mnist = {}\n",
    "\n",
    "-- download the dataset, 32x32 geometry\n",
    "mnist.path_remote = 'https://s3.amazonaws.com/torch7/data/mnist.t7.tgz'\n",
    "mnist.path_dataset = 'mnist.t7'\n",
    "mnist.path_trainset = paths.concat(mnist.path_dataset, 'train_32x32.t7')\n",
    "mnist.path_testset = paths.concat(mnist.path_dataset, 'test_32x32.t7')\n",
    "\n",
    "function mnist.download()\n",
    "   if not paths.filep(mnist.path_trainset) or not paths.filep(mnist.path_testset) then\n",
    "      local remote = mnist.path_remote\n",
    "      local tar = paths.basename(remote)\n",
    "      os.execute('wget ' .. remote .. '; ' .. 'tar xvf ' .. tar .. '; rm ' .. tar)\n",
    "   end\n",
    "end\n",
    "\n",
    "function mnist.loadTrainSet(maxLoad, geometry)\n",
    "   return mnist.loadDataset(mnist.path_trainset, maxLoad, geometry)\n",
    "end\n",
    "\n",
    "function mnist.loadTestSet(maxLoad, geometry)\n",
    "   return mnist.loadDataset(mnist.path_testset, maxLoad, geometry)\n",
    "end\n",
    "\n",
    "function mnist.loadDataset(fileName, maxLoad)\n",
    "   mnist.download()\n",
    "\n",
    "--     read dataset\n",
    "   local f = torch.load(fileName, 'ascii')\n",
    "   local data = f.data:type(torch.getdefaulttensortype())\n",
    "   local labels = f.labels\n",
    "\n",
    "--     data formatation for torch/lua\n",
    "   local nExample = f.data:size(1)\n",
    "   if maxLoad and maxLoad > 0 and maxLoad < nExample then\n",
    "      nExample = maxLoad\n",
    "      print('<mnist> loading only ' .. nExample .. ' examples')\n",
    "   end\n",
    "   data = data[{{1,nExample},{},{},{}}]\n",
    "   labels = labels[{{1,nExample}}]\n",
    "   print('<mnist> done')\n",
    "\n",
    "   local dataset = {}\n",
    "   dataset.data = data\n",
    "   dataset.labels = labels\n",
    "    \n",
    "    \n",
    "--     normalise dataset, like in the tutorial\n",
    "\n",
    "   function dataset:normalize(mean_, std_)\n",
    "      local mean = mean_ or data:view(data:size(1), -1):mean(1)\n",
    "      local std = std_ or data:view(data:size(1), -1):std(1, true)\n",
    "      for i=1,data:size(1) do\n",
    "         data[i]:add(-mean[1][i])\n",
    "         if std[1][i] > 0 then\n",
    "            tensor:select(2, i):mul(1/std[1][i])\n",
    "         end\n",
    "      end\n",
    "      return mean, std\n",
    "   end\n",
    "\n",
    "   function dataset:normalizeGlobal(mean_, std_)\n",
    "      local std = std_ or data:std()\n",
    "      local mean = mean_ or data:mean()\n",
    "      data:add(-mean)\n",
    "      data:mul(1/std)\n",
    "      return mean, std\n",
    "   end\n",
    "\n",
    "   function dataset:size()\n",
    "      return nExample\n",
    "   end\n",
    "\n",
    "   local labelvector = torch.zeros(10)\n",
    "\n",
    "--     set meta table for torch to index\n",
    "   setmetatable(dataset, {__index = function(self, index)\n",
    "\t\t\t     local input = self.data[index]\n",
    "\t\t\t     local class = self.labels[index]\n",
    "\t\t\t     local label = labelvector:zero()\n",
    "\t\t\t     label[class] = 1\n",
    "\t\t\t     local example = {input, label}\n",
    "                                       return example\n",
    "   end})\n",
    "\n",
    "   return dataset\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mnist> done\t\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<mnist> done\t\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- create training set and normalize\n",
    "trainData = mnist.loadTrainSet(nbTrainingPatches, geometry)\n",
    "trainData:normalizeGlobal(mean, std)\n",
    "-- create test set and normalize\n",
    "testData = mnist.loadTestSet(nbTestingPatches, geometry)\n",
    "testData:normalizeGlobal(mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to familiarize yourself with the data structure before you go ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you got to define and train your neural network. A good starting point would be using the architecture of the above mentioned paper (LeCun). Try to keep it simple at this point, since you will have time for further improvements later during this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require 'nn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error given by the training function is based on the training dataset. It indicates how well the network parameters fit the training data. As you know it is only half the story, since with enough parameters, one could basically fit the training data perfectly, which is then called overfitting. How well your network actually classifies is definied by its performance on the test data. Therefore write a function, which evaluates your CNN classification error on the test dataset. \n",
    "\n",
    "___Reminder___\n",
    "\n",
    "Lua's function call is by reference, so keep that in mind for the rest of the exercise! \n",
    "\n",
    "Also the following expressions are equivalent:\n",
    "\n",
    "```lua\n",
    "foo:bar()\n",
    "foo.bar(foo)\n",
    "```\n",
    "\n",
    "Also, keep in mind, function headers are given to you as an orientation. If you want to define them in a different way, that is perfectly fine as long as you get your results ;) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function eval(network, testData)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Visualization___\n",
    "\n",
    "Now, to get some intuition about what is happening in the different layers, people usually visualize different sets of parameters of the network. This makes most sense for the convolutional layers, since there the most important computation is happening. On images it is also possible to see what kind of feature each convolution layer is looking for. Try first to write a function to visualize the convolutional kernels of each convolutional layer.\n",
    "\n",
    "Be careful if you scale any images what type of interpolation you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "require 'image'\n",
    "\n",
    "function visWeights(network, layers)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For exploring more what those kernels mean, implement a function, which visualizes the convolution of the kernel with its input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function visActivation(network, example)\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did everything right, you should now observe something similar to this\n",
    "\n",
    "![alt text](http://localhost:8888/files/1st%20milestone.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Adjust your network___\n",
    "\n",
    "If you want to adjust the sizes of your layers, you need to make sure, that the output of the n-th layer can be used as an input for the (n+1)-th layer. In a CNN the size of your feature maps is influenced by many parameters, such as the kernel size, stride, size of padding, and the size of the pooling window.\n",
    "\n",
    "Try to derive a formula that computes the width/height of an output image for a convolutional and pooling layer.\n",
    "Try to write a function, that returns the output size for a given input for convolutional and pooling layers\n",
    "\n",
    "Calculate the size of the feature maps for the following given layers:\n",
    "\n",
    "```\n",
    "convolutional layer\n",
    "    input:400,400\n",
    "    kernel:15,15\n",
    "    stride:1\n",
    "    padding:0\n",
    "pooling layer:\n",
    "    input: (from conv layer)\n",
    "    kernel:2,2\n",
    "    stride:2\n",
    "    \n",
    "convolutional layer\n",
    "    input:300,300\n",
    "    kernel:7,7\n",
    "    stride:2\n",
    "    padding:0\n",
    "pooling layer:\n",
    "    input: (from conv layer)\n",
    "    kernel:4,4\n",
    "    stride:4\n",
    "    \n",
    "convolutional layer\n",
    "    input:200,200\n",
    "    kernel:5,5\n",
    "    stride:1\n",
    "    padding:0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function spatialDimension(inpWidth, kernelSize, stride, padding) \n",
    "\n",
    "end\n",
    "\n",
    "function poolingDimension(inpWidth, kernelSize, stride)\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with that information in mind, you can go ahead and define your own network,train and evaluate it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Large or small networks___\n",
    "\n",
    "\n",
    "One might argue, that one way to prevent overfitting would be to use a smaller network, since the smaller representational power does lead to better generalization. However, in practice it turns out that instead of reducing the network size, it is almost always better to work with a large network and instead use regularization, Dropout, or injected input noise.\n",
    "\n",
    "The reasoning behind that is, smaller networks are harder to train with gradient descent, since their loss function has few local minima, which are easier to converge to and with high loss. So the performance of the classification is more prone to a bad initialization of the initial, random weights. On the other hand, big neural networks contain significantly more local minima, which turn out to have a better performance, in terms of their loss. Equally, they rely less on luck of random initialization.\n",
    "If you are interested in a more in-depth discussion, please read the following paper:\n",
    "http://arxiv.org/pdf/1412.0233v3.pdf\n",
    "\n",
    "Try out for yourself, how small networks (few layers, few feature maps) compare, to large networks. Implement a single layer CNN and test it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___GPU computing___\n",
    "\n",
    "As you have probably experienced, training CNNs is computationally very expensive. GPUs are known to speed up training substantially, so it might be a good idea to train on the GPU from now on (if you have a good one :wink:).\n",
    "If you want to try that, start with getting the newest NVIDIA Cuda from: http://docs.nvidia.com/cuda/index.html\n",
    "Afterwards you might need to also install some more torch module via luarocks, i.e. luarocks install cunn! Don't forget to refresh your env variable, in case luarocks doesn't work immediately\n",
    "\n",
    "```bash\n",
    "# On Linux with bash\n",
    "source ~/.bashrc\n",
    "# On Linux with zsh\n",
    "source ~/.zshrc\n",
    "# On OSX or in Linux with none of the above.\n",
    "source ~/.profile\n",
    "```\n",
    "\n",
    "or alternativetly run can run the following, if you are running torch in a virtual environment\n",
    "\n",
    "```bash\n",
    ".../torch/install/bin/torch-activate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Receptive field size___\n",
    "\n",
    "An interesting question one might ask is why successful convolutional networks have a deep, stacked architecture.\n",
    "Imagine having 3 convolutional layers, with 3x3 filters, stacked onto each other (with non-linearities) in between. \n",
    "A neuron in the first layer would see 3x3 in the input volume. The second one would see 5x5 and the last one would see 7x7 in the input volume. Now, knowing that, one could ask, why not using a single convolutional layer with 7x7 filter. How would you answer? You can read up on that topic in the following paper. Try to give a verbal explanation.\n",
    "\n",
    " Bengio Y (2009) Learning Deep Architectures for AI.\n",
    " Found Trends Mach Learn 2: 1–127.\n",
    " doi:10.1561/2200000006.\n",
    " \n",
    " link: http://www.iro.umontreal.ca/~bengioy/papers/ftml_book.pdf\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Further improvements___\n",
    "\n",
    "Ok, now we are ready for some optimization to get a better performance on the test data. We will perform a couple of steps to improve our performance. This is a rather iterative process and you maybe have to go back and forth a couple of times in order to find out, which adjustments work well together in which ways.\n",
    "\n",
    "\n",
    "Now the following section gives you the opportunity to improve your network using state-of-the-art tricks. Try to implement them. You can build up on the network you defined earlier (which you can also edit).\n",
    "\n",
    "To have an overview of how different classifiers performed on MNIST visit: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "You are strongly advised to use as many routines from the torch framework as possible, rather then implementing from scratch.\n",
    "\n",
    "Furthermore, you should achieve at least ___98 %___ accuracy on the test data in order to receive points for this exercise.\n",
    "\n",
    "___Regularization___\n",
    "\n",
    "Now, as mentioned earlier, the network might overfit on the data. In order to avoid this and generalize better to test datasets, you need now to add regularization to your training. The classical way to do so is to add a regularization (L2) term to the loss function, which penalizes a perfect fit in each iteration. A recent, very promising approach is called Dropout, as you heard in the lecture.\n",
    "\n",
    "Implement both and compare the performance improvements.\n",
    "\n",
    "\n",
    "___Optimization___\n",
    "\n",
    "An integral part of training your neural network is the optimization algorithm you are using. There are a couple of options you can try out. To do so, please use the optim package of torch.\n",
    "\n",
    "- Stochastic Gradient Descent with\n",
    "\n",
    "    - learning rate decay\n",
    "    - momentum\n",
    "\n",
    "- Adam learning: http://arxiv.org/pdf/1412.6980.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require 'optim'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Batch normalization___\n",
    "\n",
    "As mentioned in the beginning, your training data is normalized to have a mean of 0 and std of 1. The problem is, that there is a shift of that happening between some layers of your network as learning progresses and weights change. Please read the following paper on why that is a problem and then implement Batch Normalization to circumvent.\n",
    "\n",
    "http://arxiv.org/pdf/1502.03167v3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Data augmentation (Optional)___\n",
    "\n",
    "In most machine learning problems a rule of thumb is \"the more data the better\". Accordingly, we would always strive for a bigger dataset. This we can achieve by augmenting our data. For that we use modifications, which are sensible in the context of the data. So, MNIST is, as you know, a dataset that consists of hand-written digits. Therefore, we want to implement augmentations, which could realistically occur for hand-written digits. This means operations like\n",
    "\n",
    "- rotation\n",
    "- scaling\n",
    "- translation\n",
    "- distortions\n",
    "- noise (speckles, lines, etc.)\n",
    "- and much more (be creative)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
